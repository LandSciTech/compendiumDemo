---
title: "Reproducible Project Workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Reproducible Project Workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  eval = FALSE,
  comment = "#>"
)
```


A reproducible project contains all the code, data, packages version and operating system information needed to run the analysis. The package [rcompendium](https://frbcesab.github.io/rcompendium/) provides tools to make it easy to incorporate best practices for reproducible research into your typical project workflow. Below we demonstrate how to set up a compendium and then go into details on some of the advanced tools used. 

## Using a Research Compendium

Check that your Git credentials are stored on your computer. If they are not see the [Git tutorial](https://landscitech.github.io/Github_tutorial/setup.html) to get setup.

```{r}
gh::gh_whoami()
```

If this is your first time using rcompendium you can set your information in the Rprofile using the `set_credentials()` function so that the next time you create a compendium these fields will be auto filled. Run the code below with your name and other info and follow the instructions. 

```{r}
rcompendium::set_credentials(given = "Jane", family = "Doe", 
                             email = "jane.doe@ec.gc.ca", 
                             orcid = "0000-0000-0000-0000")
```

Use usethis or the Rstudio GUI to create a new project in a place that will be easy to delete later. Note that the name of the folder will be the name of your project and GitHub repo so make it informative and stable because it is annoying to change later. 
```{r}
usethis::create_project("~/Desktop/testCompendium")
```

In the new project which should have just opened create a new compendium. 
```{r}
rcompendium::new_compendium(create_repo = TRUE, renv = TRUE)
```

This adds many files and folders to your project based on a standard folder structure. You will need to edit the README.Rmd to describe the project and explain what you will use each folder for. Recommended descriptions of the default folders are:

  - ⁠data⁠: a folder to store raw and derived data. Note that these data must never be modified. If user want to modify them it is recommended to export new data in ⁠outputs⁠.
  
  - ⁠analyses⁠: a folder to write analyses instructions, i.e. R scripts. If user need to create R functions it is recommended to write them in the ⁠R folder.
 
  - ⁠outputs⁠: a folder to store intermediate and final outputs generated by the R scripts.

  - ⁠figures⁠: a folder to store figures generated by the R scripts.

You will also need to edit the DESCRIPTION Title and Description items and potentially add additional authors. 

The rest of the files and folders are boilerplate many of which can be customized by changing the arguments to `rcompendium::new_compendium`. 

Now we are ready to start some data analysis. Create a blank R script in the analyses folder with a number to indicate the order it should be run in eg "analyses/01_run_model.R". Our goal in this analysis is to determine which variables in the mtcars data set is the best predictor of mpg. To do this we build a linear model for each variable vs mpg. A first attempt might be to right them all out by hand.

```{r}
  mod_cyl <- lm(mpg ~ cyl, data = mtcars)
  gl_cyl <- broom::glance(mod_cyl)
  
  mod_disp <- lm(mpg ~ disp, data = mtcars)
  gl_disp <- broom::glance(mod_disp)
  
  data.frame(var = c("cyl", "disp"), r.squared = c(gl_cyl$r.squared, gl_disp$r.squared))
  # ... for all the variables... sigh
```

But it will be much faster and harder to make mistakes if we use a function and iterate over each variable name. 

```{r}
do_mod <- function(x){
  form <- as.formula(paste0("mpg ~ ", x))
  mod <- lm(form, data = mtcars)
  gl <- broom::glance(mod)
  
  data.frame(variable = x, r.squared = gl$r.squared)
}

r2_tab <- purrr::map_dfr(names(mtcars)[-1], do_mod) |> 
  dplyr::mutate(variable = reorder(variable, r.squared))

r2_tab
```

Here I have used `packagename::function()` instead of loading the whole package for purrr, dplyr and broom because it is more explicit and makes it clear where each function comes from. If you want to avoid typing the package name you can load the function you need with `usethis::use_import_from("packagname", "function")` which adds `#' @importFrom package function` to the package documentation file ("R/testCompendium-package.R"). If you want the whole package to be loaded add `#' @import package` to the same file and then run `devtools::document()`. This ensures that the scripts work the same anytime the project is loaded with `devtools::load_all()` rather than being sensitive to which `library(package)` calls have happened and in which order. This is the recommended practice but if you use `library(package)` it will usually still work.   

Having the function definition in the R script is fine for this simple function but if we want to use the function in multiple scripts or take advantage of R package development tools to automatically load all our functions and have the option to test and document those functions we need to put the function definition in the R folder. You can create a new file manually called "R/do_mod.R" or you can use `usethis::use_r("do_mod")` to create and open the file. Then copy the function definition to the file. Next we want to document the function so that the inputs and outputs and any assumptions of the function are clear. To do so using roxygen2 comments click inside the function definition and then click Code>Insert Roxygen Skeleton from the RStudio menu. Fill in the title followed by a description of what the function does and then a description of each parameter and the value that is returned. Now run `devtools::document()` and then ?do_mod to see the documentation for the function. See the R packages chapter [Function Documentation](https://r-pkgs.org/man.html) for details. 

```{r}
#' Run the model for any variable
#'
#' Run a linear model of the variable vs mpg for the mtcars data
#' set and extract the r squared
#'
#' @param x variable name from mtcars
#'
#' @return data.frame with the variable name and r.squared value
#'
#' @export
#' 
do_mod <- function(x){
  form <- as.formula(paste0("mpg ~ ", x))
  mod <- lm(form, data = mtcars)
  gl <- broom::glance(mod)
  
  data.frame(variable = x, r.squared = gl$r.squared)
}

```

Now if you run `devtools::load_all()` all the code in testCompendium/R will be sourced and the functions will be available as if it was a package that you loaded. Also packages listed with `# @imports` or `# @importFrom` will be loaded.  

At this stage we should have two scripts "R/do_mod.R" that contains the function definition and "analyses/01_run_model.R" with the code to analyze the mtcars data. Now we can add another script to the analyses folder where we will make a figure of our results, "analyses/02_make_fig.R". And save the following code. I also add `# @imports ggplot2` to the "R/testCompendium-package.R" script so that I don't need to write `ggplot2::` all the time.

```{r}
ggplot(r2_tab, aes(variable, r.squared))+
  geom_col()

ggsave("figures/r2_mpg_variables.png")
```

Once the scripts are ready I the following to "make.R" file that will source each script and also set a global ggplot2 theme. 

```{r}
## Global Variables ----

# You can list global variables here (or in a separate R script)

# set a global ggplot theme
theme_set(theme_classic())

## Run Project ----

# List all R scripts in a sequential order and using the following form:
source(here::here("analyses", "01_run_model.R"))
source(here::here("analyses", "02_make_fig.R"))
```

To ensure that all dependencies have been recorded run `rcompendium::add_dependencies(".")`.

Finally, restart R and then run `source("make.R")`. This will ensure that your script works from a fresh R session. If everything works as expected update the renv lockfile by running `renv::snapshot()` to ensure you capture the state when it is working.




