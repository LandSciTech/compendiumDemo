---
title: "Reproducible Project Workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Reproducible Project Workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  eval = FALSE,
  comment = "#>"
)
```


A reproducible project contains all the code, data, packages version and operating system information needed to run the analysis. The package [rcompendium](https://frbcesab.github.io/rcompendium/) provides tools to make it easy to incorporate best practices for reproducible research into your typical project workflow. Below we demonstrate how to set up a compendium and then go into details on some of the advanced tools used. 

## Using a Research Compendium

Check that your Git credentials are stored on your computer. If they are not see the [Git tutorial](https://landscitech.github.io/Github_tutorial/setup.html) to get setup.

```{r}
gh::gh_whoami()
```

If this is your first time using rcompendium you can set your information in the Rprofile using the `set_credentials()` function so that the next time you create a compendium these fields will be auto filled. Run the code below with your name and other info and follow the instructions. 

```{r}
rcompendium::set_credentials(given = "Jane", family = "Doe", 
                             email = "jane.doe@ec.gc.ca", 
                             orcid = "0000-0000-0000-0000")
```

Use usethis or the Rstudio GUI to create a new project in a place that will be easy to delete later. Note that the name of the folder will be the name of your project and GitHub repo so make it informative and stable because it is annoying to change later. 
```{r}
usethis::create_project("~/Desktop/testCompendium")
```

In the new project which should have just opened create a new compendium. 
```{r}
rcompendium::new_compendium(create_repo = TRUE, renv = TRUE)
```

This adds many files and folders to your project based on a standard folder structure. You will need to edit the README.Rmd to describe the project and explain what you will use each folder for. Recommended descriptions of the default folders are:

  - ⁠data⁠: a folder to store raw and derived data. Note that these data must never be modified. If users want to modify them it is recommended to export new data in ⁠outputs⁠.
  
  - ⁠analyses⁠: a folder to write analyses instructions, i.e. R scripts. If user need to create R functions it is recommended to write them in the ⁠R folder.
 
  - ⁠outputs⁠: a folder to store intermediate and final outputs generated by the R scripts.

  - ⁠figures⁠: a folder to store figures generated by the R scripts.

You will also need to edit the DESCRIPTION Title and Description items and potentially add additional authors. 

The rest of the files and folders are boilerplate many of which can be customized by changing the arguments to `rcompendium::new_compendium`. 

Now we are ready to start some data analysis. Create a blank R script in the analyses folder with a number to indicate the order it should be run in eg "analyses/01_run_model.R". Our goal in this analysis is to determine which variables in the mtcars data set is the best predictor of mpg. To do this we build a linear model for each variable vs mpg. A first attempt might be to right them all out by hand.

```{r}
  mod_cyl <- lm(mpg ~ cyl, data = mtcars)
  gl_cyl <- broom::glance(mod_cyl)
  
  mod_disp <- lm(mpg ~ disp, data = mtcars)
  gl_disp <- broom::glance(mod_disp)
  
  data.frame(var = c("cyl", "disp"), r.squared = c(gl_cyl$r.squared, gl_disp$r.squared))
  # ... for all the variables... sigh
```

But it will be much faster and harder to make mistakes if we use a function and iterate over each variable name. 

```{r}
do_mod <- function(x){
  form <- as.formula(paste0("mpg ~ ", x))
  mod <- lm(form, data = mtcars)
  gl <- broom::glance(mod)
  
  data.frame(variable = x, r.squared = gl$r.squared)
}

r2_tab <- purrr::map_dfr(names(mtcars)[-1], do_mod) |> 
  dplyr::mutate(variable = reorder(variable, r.squared))

r2_tab
```

Here I have used `packagename::function()` instead of loading the whole package for purrr, dplyr and broom because it is more explicit and makes it clear where each function comes from. If you want to avoid typing the package name you can load the function you need with `usethis::use_import_from("packagname", "function")` which adds `#' @importFrom package function` to the package documentation file ("R/testCompendium-package.R"). If you want the whole package to be loaded add `#' @import package` to the same file and then run `devtools::document()`. This ensures that the scripts work the same anytime the project is loaded with `devtools::load_all()` rather than being sensitive to which `library(package)` calls have happened and in which order. This is the recommended practice but if you use `library(package)` it will usually still work.   

Having the function definition in the R script is fine for this simple function but if we want to use the function in multiple scripts or take advantage of R package development tools to automatically load all our functions and have the option to test and document those functions we need to put the function definition in the R folder. You can create a new file manually called "R/do_mod.R" or you can use `usethis::use_r("do_mod")` to create and open the file. Then copy the function definition to the file. Next we want to document the function so that the inputs and outputs and any assumptions of the function are clear. To do so using roxygen2 comments click inside the function definition and then click Code>Insert Roxygen Skeleton from the RStudio menu. Fill in the title followed by a description of what the function does and then a description of each parameter and the value that is returned. Now run `devtools::document()` and then ?do_mod to see the documentation for the function. See the R packages chapter [Function Documentation](https://r-pkgs.org/man.html) for details. 

```{r}
#' Run the model for any variable
#'
#' Run a linear model of the variable vs mpg for the mtcars data
#' set and extract the r squared
#'
#' @param x variable name from mtcars
#'
#' @return data.frame with the variable name and r.squared value
#'
#' @export
#' 
do_mod <- function(x){
  form <- as.formula(paste0("mpg ~ ", x))
  mod <- lm(form, data = mtcars)
  gl <- broom::glance(mod)
  
  data.frame(variable = x, r.squared = gl$r.squared)
}

```

Now if you run `devtools::load_all()` all the code in testCompendium/R will be sourced and the functions will be available as if it was a package that you loaded. Also packages listed with `# @imports` or `# @importFrom` will be loaded.  

At this stage we should have two scripts "R/do_mod.R" that contains the function definition and "analyses/01_run_model.R" with the code to analyze the mtcars data. Now we can add another script to the analyses folder where we will make a figure of our results, "analyses/02_make_fig.R". And save the following code. I also add `# @imports ggplot2` to the "R/testCompendium-package.R" script so that I don't need to write `ggplot2::` all the time.

```{r}
ggplot(r2_tab, aes(variable, r.squared))+
  geom_col()

ggsave("figures/r2_mpg_variables.png")
```

Once the scripts are ready I add the following to the bottom of the "make.R" file that was created with default values when we created the compendium. This will source each script and also set a global ggplot2 theme. 

```{r}
## Global Variables ----

# You can list global variables here (or in a separate R script)

# set a global ggplot theme
theme_set(theme_classic())

## Run Project ----

# List all R scripts in a sequential order and using the following form:
source(here::here("analyses", "01_run_model.R"))
source(here::here("analyses", "02_make_fig.R"))
```

The make.R file also runs `renv::restore()` which will install the versions of the R packages recorded in the lockfile. To update the versions in the lockfile run `renv::snapshot()`. To ensure that all dependencies have been recorded in the DESCRIPTION run `rcompendium::add_dependencies(".")`.

Finally, restart R and then run `source("make.R")`. This will ensure that your scripts work from a fresh R session. Then commit and push all your changes to GitHub. Now a collaborator should be able to clone your repo from GitHub and run `source("make.R")` to reproduce your analysis. 

## Managing data

Above we used a built in data set but typically you will have some data that you want to store in the data folder. If the data is less than 100 Mb and is recorded in a text or csv file it can be tracked in GitHub and can be included in the repo. Otherwise you will want to store the data somewhere other than GitHub. 

To stop git from tracking things stored in the data folder you can add "data/" to a new line in the .gitignore file in the project directory (`usethis::edit_git_ignore("project")` will open the file for you). This means that the data folder will not be tracked by git and will not be uploaded to the repo on GitHub so you will need to have an alternative mechanism for collaborators to access the data. The simplest option is to add a note in the project README that explains where to manually download the data or who to contact to get the data. To have a more automated solution you will need to store the data somewhere online. Two options with R packages are Google Drive ([googledrive](https://googledrive.tidyverse.org/)) and OSF ([osfr](https://docs.ropensci.org/osfr/)). To use googledrive for example you would need to have the data available in a Google Drive folder that is shared with the user. Then you can include something like the below as the first step in your data analysis. 

```{r}
# get raw lit review data from google drive
gd_url <- "https://docs.google.com/spreadsheets/d/16X9oxbmufCTy8WpV2zSj9KoVmImkXa5Mz-KGGMBqIkk/edit?usp=sharing"

# file path where data is saved
fl_pth <- here::here("data/raw-data", "data_from_GD.xlsx")

# running this will prompt you for permission to connect to your google drive
googledrive::drive_auth(email = TRUE)

googledrive::drive_download(
  googledrive::as_id(gd_url),
  path = fl_pth,
  overwrite = TRUE)

raw_lr_dat <- readxl::read_xlsx(fl_pth)
```

OSF (Open Science Framework) is better than Google drive if you want a more polished place to store data that you will publish. Creating an OSF project creates a private or public website that can be used with collaborators or the public. For an example see the OSF page for the [CAN-SAR Database](https://osf.io/e4a58/). You can archive an OSF project with a doi to reference a specific version in a paper (called a "registration"). 
## Writing a paper
You can use Rmarkdown or Quarto to author papers. The main benefit is the ability to include figures, tables and in text code in the paper so that you never need to update numbers or copy paste figures in a paper after revising the analysis. It also automatically formats citations based of a csl file so you can change citation formats quickly by just changing files. 






